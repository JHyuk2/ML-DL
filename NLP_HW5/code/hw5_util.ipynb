{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from PyKomoran import *\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import *\n",
    "\n",
    "#   *** Do not modify the code ***\n",
    "class NLP_util:\n",
    "    def __init__(self):\n",
    "        self.labels = sorted(['opening', 'request', 'wh-question', 'yn-question', 'inform', 'affirm', 'ack', 'expressive'])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.komoran = Komoran('EXP')\n",
    "        self.word2idx = dict()\n",
    "        self.labels2idx = dict()\n",
    "        self.idx2labels = dict()\n",
    "        self.data_dict = None\n",
    "        self.test_dataset = None\n",
    "        self.train_dataset = None\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def convert_examples_to_features(self, data):\n",
    "        return\n",
    "\n",
    "    def create_w2i_l2i_i2l(self, data):\n",
    "        return\n",
    "        \n",
    "    def make_dataset(self, input_ids, labels):\n",
    "        return torch.utils.data.TensorDataset(torch.tensor(input_ids, dtype=torch.long), torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "    def load_data(self, input_path=None):\n",
    "        assert type(input_path) is str, '입력 파일 경로(input_path)를 확인하세요.'\n",
    "\n",
    "        with open(input_path, mode='r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            if input_path.split('_')[-1][:-5] == 'te':\n",
    "                self.data_dict = data\n",
    "                input_ids, labels = self.convert_examples_to_features(data)\n",
    "                self.test_dataset = self.make_dataset(input_ids, labels)\n",
    "            elif input_path.split('_')[-1][:-5] == 'tr':\n",
    "                self.create_w2i_l2i_i2l(data)\n",
    "                input_ids, labels = self.convert_examples_to_features(data)\n",
    "                self.train_dataset = self.make_dataset(input_ids, labels)\n",
    "            else:\n",
    "                raise ValueError('입력파일이 train 파일인지, test 파일인지 확인이 불가능합니다.')\n",
    "\n",
    "    def save_result(self, result, std_name, std_id):\n",
    "        with open('./'+str(std_name)+'_'+str(std_id)+'_hw5.txt', mode='w', encoding='utf-8') as f:\n",
    "            f.write('********** Eval Result **********\\n')\n",
    "            f.write('Macro averaging precision: {:.2f}%\\n'.format(result['macro precision']))\n",
    "            f.write('Micro averaging precision: {:.2f}%\\n\\n'.format(result['micro precision']))\n",
    "\n",
    "            f.write('Macro averaging recall: {:.2f}%\\n'.format(result['macro recall']))\n",
    "            f.write('Micro averaging recall: {:.2f}%\\n\\n'.format(result['micro recall']))\n",
    "\n",
    "            f.write('Macro averaging f1-score: {:.2f}%\\n'.format(result['macro f1']))\n",
    "            f.write('Micro averaging f1-score: {:.2f}%\\n\\n'.format(result['micro f1']))\n",
    "\n",
    "            f.write('Dialogue Number: {}'.format(result['dialogue number']))\n",
    "            \n",
    "            for Utterance in result['matching']:\n",
    "                f.write('\\n\\nUtterance: {}'.format(Utterance[0]))\n",
    "                f.write('\\nReal label: {}'.format(Utterance[1]))\n",
    "                f.write('\\nPredicted label: {}'.format(Utterance[2]))\n",
    "                f.write('\\nResult: {}'.format(Utterance[3]))\n",
    "        f.close()\n",
    "    \n",
    "    def check_optimizer(self, model, optimizer_name, learning_rate):\n",
    "        if optimizer_name == 'adam':\n",
    "            return torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == 'adamw':\n",
    "            return torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == 'rmsprop':\n",
    "            return torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == 'sgd':\n",
    "            return torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            raise ValueError('optimizer_name이 pytorch에 존재하지 않습니다. 다시 확인하세요.')\n",
    "    \n",
    "    def check_loss_ft(self, loss_ft):\n",
    "        if loss_ft == 'mseloss':\n",
    "            return torch.nn.MSELoss()\n",
    "        elif loss_ft == 'crossentropyloss':\n",
    "            return torch.nn.CrossEntropyLoss()\n",
    "        elif loss_ft == 'nllloss':\n",
    "            return torch.nn.NLLLoss()\n",
    "        else:\n",
    "            raise ValueError('loss_function이 pytorch에 존재하지 않습니다. 다시 확인하세요.')\n",
    "\n",
    "    def train(self, model, loss_ft, optimizer_name, learning_rate, train_batch_size, num_train_epochs):\n",
    "        # optimizer\n",
    "        assert type(optimizer_name) is str, 'optimizer_name의 type은 string이 되어야 합니다.'\n",
    "        optimizer = self.check_optimizer(model, optimizer_name.lower(), learning_rate)\n",
    "\n",
    "        # Loss function\n",
    "        assert type(loss_ft) is str, 'loss_ft type은 string이 되어야 합니다.'\n",
    "        criterion = self.check_loss_ft(loss_ft.lower())\n",
    "\n",
    "        self.set_seed(42)\n",
    "        \n",
    "        train_DataLoader = torch.utils.data.DataLoader(self.train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "        train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "\n",
    "        print(\"\\n***** Running training *****\")\n",
    "        print(\"  Num examples = {}\".format(len(self.train_dataset)))\n",
    "        print(\"  Num Epochs = {}\".format(num_train_epochs))\n",
    "        print(\"  Train Batch size = {}\".format(train_batch_size))\n",
    "        print(\"  Device = \", self.device)\n",
    "\n",
    "        model.to(self.device)\n",
    "        model.train(True)\n",
    "        model.zero_grad()\n",
    "        for epoch in train_iterator:\n",
    "            loss = 0\n",
    "            for batch in train_DataLoader:\n",
    "                input_vector = batch[0].to(self.device)\n",
    "                label = batch[1].to(self.device)\n",
    "                predict = model(input_vector)\n",
    "\n",
    "                loss = criterion(predict, label)\n",
    "                loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "            if (epoch+1) % 50 == 0:\n",
    "                print(\"\\n********** Train Result **********\")\n",
    "                print(\"  Epoch / Total Epoch : {} / {}\".format(epoch + 1, num_train_epochs))\n",
    "                print(\"  Loss : {:.4f}\".format(loss))\n",
    "                \n",
    "        model.train(False)\n",
    "\n",
    "    def predict(self, model):\n",
    "        test_DataLoader = torch.utils.data.DataLoader(self.test_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "        print(\"***** Running Prediction *****\")\n",
    "        print(\"  Num examples = {}\".format(len(self.test_dataset)))\n",
    "        print(\"  Test Batch size = 1\")\n",
    "\n",
    "        model.eval()\n",
    "        pred = None\n",
    "        label = None\n",
    "        for batch in test_DataLoader:\n",
    "            input_vector = batch[0].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predict = model(input_vector)\n",
    "            \n",
    "            if pred is None:\n",
    "                pred = predict.detach().cpu().numpy()\n",
    "                label = batch[1].numpy()\n",
    "            else:\n",
    "                pred = np.append(pred, predict.detach().cpu().numpy(), axis=0)\n",
    "                label = np.append(label, batch[1].numpy(), axis=0)\n",
    "\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "\n",
    "        print(\"***** Prediction 완료 *****\")\n",
    "\n",
    "        return pred.tolist(), label.tolist()\n",
    "    \n",
    "    def eval(self, pred, label, dialogue_number):\n",
    "        return\n",
    "#   *** Do not modify the code ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit5992200e62d14719bcdeb39d5720987a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
