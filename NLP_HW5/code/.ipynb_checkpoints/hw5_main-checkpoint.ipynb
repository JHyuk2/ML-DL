{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from hw5_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_dim = output_dim\n",
    "        self.kernel_size_list = [3, 4, 5]\n",
    "        self.num_filter = 30\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dims=128,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        \n",
    "        # Conv Layers\n",
    "        self.conv_blocks = []\n",
    "        \n",
    "        for sz in kernel_size_list:\n",
    "            conv = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(in_channels=128, out_channels=30, kernel_size=sz, strides = 1),\n",
    "                torch.nn.MaxPool1d(1, strides=1),\n",
    "                torch.nn.flatten()\n",
    "            )\n",
    "            self.conv_blocks.append(conv)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = torch.nn.Linear(90, output_dim, bias=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeded = self.embed(inputs)\n",
    "        \n",
    "        # convolution & pooling\n",
    "        conved = [self.conv_blocks[i](embeded) for i in range(3)]\n",
    "        \n",
    "        # concatenate\n",
    "        concated = torch.cat([conved[i] for i in range(3)], dim=0)\n",
    "        \n",
    "        #output\n",
    "        output = self.fc(concated)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --dialogue-number\n",
      "                             {261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300}\n",
      "ipykernel_launcher.py: error: the following arguments are required: --dialogue-number\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# train_path = '../data/SpeechAct_tr.json'\n",
    "# test_path = '../data/SpeechAct_te.json'\n",
    "\n",
    "class CNN_util(NLP_util):  \n",
    "    def create_w2i_l2i_i2l(self, data):\n",
    "        # 1) word2idx, idx2word 구축\n",
    "        word_list = []\n",
    "        word_idx = 2\n",
    "        label_idx = 0\n",
    "        self.idx2word = dict()\n",
    "        self.labels2idx = dict()\n",
    "        \n",
    "        for k in data.keys():\n",
    "            for sentence_list in data[k]:\n",
    "                contents = sentence_list[1]\n",
    "                tmp_list = self.komoran.get_list(contents).split()\n",
    "                word_list += tmp_list\n",
    "                \n",
    "        word_set = sorted(set(word_list))\n",
    "        \n",
    "        # 반드시 0번인덱스는 PAD, 1번 UNK\n",
    "        self.word2idx['<PAD>'] = 0\n",
    "        self.word2idx['<UNK>'] = 1\n",
    "        self.idx2word[0] = '<PAD>'\n",
    "        self.idx2word[1] = '<UNK>'\n",
    "        \n",
    "        for word in word_set:    \n",
    "            self.word2idx[word] = word_idx\n",
    "            self.idx2word[word_idx] = word\n",
    "            word_idx += 1\n",
    "        \n",
    "        # 2) labels2idx\n",
    "        for label in self.labels:\n",
    "            self.labels2idx[label] = label_idx\n",
    "            self.idx2labels[label_idx] = label\n",
    "            label_idx +=1\n",
    "            \n",
    "        \"\"\"\n",
    "            *** 입력된 train data를 사용하여 self.word2idx를 구축하고, self.labels를 사용하여 self.labels2idx, self.idx2labels을 구축하시면 됩니다. ***\n",
    "\n",
    "            1) data는 train_data로, 아래와 같이 dict 자료형으로 구성되어 있습니다.\n",
    "                {\n",
    "                    \"1\":[\n",
    "                            [\n",
    "                                \"user\",\n",
    "                                \"아름아 그동안 잘 있었어?\",\n",
    "                                \"opening\"\n",
    "                            ],\n",
    "                            ...생략...\n",
    "                        ]\n",
    "                    \n",
    "                    ...생략...\n",
    "                }\n",
    "            \n",
    "            2) self.komoran을 사용하여 입력된 모든 발화를 형태소 분석한 후, word2idx를 구축하여, self.word2idx에 저장하시면 됩니다.\n",
    "\n",
    "            3) self.labels는 아래와 같이 list 자료형이며, 화행 분석의 모든 레이블이 있습니다.\n",
    "                self.labesl = ['ack', 'affirm', ... , 'yn-question']\n",
    "\n",
    "            4) self.labels를 사용하여, labels2idx 및 idx2labels을 구축하여, 각각 self.labels2idx와 idx2labels에 저장하시면 됩니다.\n",
    "        \"\"\"\n",
    "        \n",
    "        return\n",
    "\n",
    "    def convert_examples_to_features(self, data, max_sequence_length=50):\n",
    "        input_ids = list()\n",
    "        labels = list()\n",
    "        \n",
    "        for k in data.keys():\n",
    "            for sentence_list in data[k]:\n",
    "                contents = sentence_list[1]\n",
    "                label = sentence_list[2]\n",
    "                tmp_list = self.komoran.get_plain_text(contents).split()\n",
    "                idx_list = []\n",
    "                \n",
    "                # max_sequence_length 조절\n",
    "                if len(tmp_list) > max_sequence_length:\n",
    "                    tmp_list = tmp_list[:max_sequence_length]\n",
    "                    \n",
    "                for idx, word in enumerate(tmp_list):\n",
    "                    idx_list.append(self.word2idx[word])        \n",
    "                else:\n",
    "                    if idx < max_sequence_length:\n",
    "                        idx_list += [0]* (max_sequence_length-1 - idx)\n",
    "\n",
    "                input_ids.append(idx_list)\n",
    "                labels.append(self.labels2idx[label])\n",
    "                \n",
    "        \"\"\"\n",
    "            *** self.word2idx, self.labels2idx를 사용하여 입력된 data의 발화와 label을 index형태로 변환하시면 됩니다. ***\n",
    "\n",
    "            1) data는 train_data 또는 test_data로, 아래와 같이 dict 자료형으로 구성되어 있습니다.\n",
    "                {\n",
    "                    \"1\":[\n",
    "                            [\n",
    "                                \"user\",\n",
    "                                \"아름아 그동안 잘 있었어?\",\n",
    "                                \"opening\"\n",
    "                            ],\n",
    "                            ...생략...\n",
    "                        ]\n",
    "                    \n",
    "                    ...생략...\n",
    "                }\n",
    "\n",
    "            2) 구축된 self.word2idx, self.labels2idx를 사용하여 입력된 data의 발화와 label을 index 형태로 변환하시면 됩니다.\n",
    "\n",
    "            3) input_ids는 2차원 list로, 입력된 data의 모든 발화를 index 형태로 변환하여 return 하시면 됩니다.\n",
    "\n",
    "            4) labels는 1차원 list로, 입력된 data의 모든 발화의 label을 index 형태로 변환하여 return 하시면 됩니다.\n",
    "\n",
    "            5) 입력된 문장의 sequence 길이를 맞춰주기 위해 본 과제에서는 max_sequence_length를 50 으로 설정하며,\n",
    "               입력된 문장의 sequence 길이가 max_sequence_length 보다 짧은 경우, \"<PAD>\"을 사용하여 max_sequence_length을 맞춘다.\n",
    "        \"\"\"\n",
    "        return input_ids, labels\n",
    "\n",
    "    def eval(self, pred, label, dialogue_number):\n",
    "        result = dict()\n",
    "        \"\"\"\n",
    "            *** 입력된 값을 사용하여 모델을 성능을 평가하시면 됩니다. ***\n",
    "\n",
    "            1) pred 변수는 모델이 예측한 label의 index로 구성된 1차원 리스트 입니다.\n",
    "                [0, 2, 5, ..., 2]\n",
    "\n",
    "            2) label 변수는 실제 정답 label로 구성된 1차원 리스트 입니다.\n",
    "                [3, 2, 5, ..., 6]\n",
    "\n",
    "            3) self.labels는 아래와 같이 화행 분석의 모든 레이블이 있습니다.\n",
    "                self.labesl = ['ack', 'affirm', ... , 'yn-question']\n",
    "\n",
    "            1, 2, 3)과 사전에 구축한 self.idx2labels을 사용하여 evaluation을 진행하시면 됩니다.\n",
    "\n",
    "            4) self.data_dict는 test_data로, 아래와 같이 dict 자료형으로 구성되어 있습니다.\n",
    "                {\n",
    "                    \"261\":[\n",
    "                                [\n",
    "                                    \"user\",\n",
    "                                    \"아름아 그동안 잘 있었어?\",\n",
    "                                    \"opening\"\n",
    "                                ],\n",
    "                                ...생략...\n",
    "                          ]\n",
    "                    \n",
    "                    ...생략...\n",
    "                }\n",
    "\n",
    "            5) argument로 입력받은 대화 번호(dialogue_number)를 사용하시면 됩니다.\n",
    "\n",
    "            6) result 변수는 아래와 같이 만드시면 됩니다.\n",
    "               ** 모델의 성능 부분을 백분율로 만든 후, result에 저장해야 됩니다. **\n",
    "\n",
    "                result['micro precision'] = 모델의 micro averaging precision (float 자료형)\n",
    "                result['macro precision'] = 모델의 macro averaging precision (float 자료형)\n",
    "\n",
    "                result['micro recall'] = 모델의 micro averaging recall (float 자료형)\n",
    "                result['macro recall'] = 모델의 macro averaging recall (float 자료형)\n",
    "\n",
    "                result['micro f1'] = 모델의 micro averaging F1-score (float 자료형)\n",
    "                result['macro f1'] = 모델의 macro averaging F1-score (float 자료형)\n",
    "\n",
    "                result['dialogue number'] = 입력 받은 dialogue_number (int 자료형)\n",
    "\n",
    "                result['matching'] = 입력 받은 dialogue_number의 모든 발화에 대한 매칭 결과가 tuple 자료형으로 되어있어야 합니다. (list 자료형)\n",
    "                Ex) result['matching'] = [('아름아 잘 잤니?', 'opening', 'opening', 'True'),  \n",
    "                                          ('아름아 일정 확인 좀 해줘', 'request', 'wh-question', 'False'),\n",
    "                                          ...생략...\n",
    "                                          ('해당 발화', '실제 레이블', '예측 레이블', '실제 레이블과 예측 레이블 비교 결과')\n",
    "                                         ]\n",
    "\n",
    "                result['matching']의 경우 해당 대화의 발화순으로 되어 있어야 합니다.\n",
    "\n",
    "            모든 소수 계산과정에서 반올림, 올림, 버림은 하지 않습니다.\n",
    "        \"\"\"\n",
    "        return result\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dialogue-number\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "        dest=\"dialogue_number\",\n",
    "#         dest=263,\n",
    "        choices=range(261, 301)\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 입력 JSON 파일 경로\n",
    "    train_inpath = '../SpeechAct_tr.json' \n",
    "    test_inpath = '../SpeechAct_te.json'\n",
    "\n",
    "    # ======= [여기를 수정하세요] =======\n",
    "    std_name = '이종혁'\n",
    "    std_ID = '2020710187'\n",
    "\n",
    "    \"\"\"\n",
    "    * HyperParameter *\n",
    "\n",
    "        loss_function_name은 string 자료형으로 다음 중 하나를 선택하시면 됩니다.\n",
    "        ['MSELoss', 'CrossEntropyLoss', 'NLLLoss']\n",
    "\n",
    "        optimizer_name string 자료형으로 다음 중 하나를 선택하시면 됩니다.\n",
    "        ['Adam', 'AdamW', 'RMSprop', 'SGD']\n",
    "\n",
    "        learning_rate는 float 자료형으로 입력하시면 됩니다.\n",
    "\n",
    "        train_batch_size는 int 자료형으로 입력하시면 됩니다.\n",
    "\n",
    "        num_train_epochs는 int 자료형으로 입력하시면 됩니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_function_name = 'crossentropyloss'\n",
    "    optimizer_name = 'adam'\n",
    "    learning_rate = 0.01\n",
    "    train_batch_size = 20\n",
    "    num_train_epochs = 50\n",
    "    # ======= [여기까지 수정하세요] =======\n",
    "\n",
    "    processing = CNN_util()\n",
    "    processing.load_data(train_inpath)\n",
    "    processing.load_data(test_inpath)\n",
    "    model = CNN(len(processing.word2idx), len(processing.labels))\n",
    "    processing.train(model, loss_function_name, optimizer_name, learning_rate, train_batch_size, num_train_epochs)\n",
    "    pred, label = processing.predict(model)\n",
    "    print(pred, label)\n",
    "#     result = processing.eval(pred, label, args.dialogue_number)\n",
    "#     processing.save_result(result, std_name, std_ID)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
