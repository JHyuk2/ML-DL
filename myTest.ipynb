{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7  2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [[1, 2],\n",
    "     [2, 1]\n",
    "    ]\n",
    "\n",
    "b = [\n",
    "    [4, 1], \n",
    "    [2, 2]]\n",
    "\n",
    "print(np.cross(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[1, 2, 3],\n",
    "     [4, 5, 6]\n",
    "    ]\n",
    "np.array(c).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive search over specified parameter values for an estimator.\n",
      "\n",
      "    Important members are fit, predict.\n",
      "\n",
      "    GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      "    It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      "    \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      "    estimator used.\n",
      "\n",
      "    The parameters of the estimator used to apply these methods are optimized\n",
      "    by cross-validated grid-search over a parameter grid.\n",
      "\n",
      "    Read more in the :ref:`User Guide <grid_search>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : estimator object.\n",
      "        This is assumed to implement the scikit-learn estimator interface.\n",
      "        Either estimator needs to provide a ``score`` function,\n",
      "        or ``scoring`` must be passed.\n",
      "\n",
      "    param_grid : dict or list of dictionaries\n",
      "        Dictionary with parameters names (string) as keys and lists of\n",
      "        parameter settings to try as values, or a list of such\n",
      "        dictionaries, in which case the grids spanned by each dictionary\n",
      "        in the list are explored. This enables searching over any sequence\n",
      "        of parameter settings.\n",
      "\n",
      "    scoring : string, callable, list/tuple, dict or None, default: None\n",
      "        A single string (see :ref:`scoring_parameter`) or a callable\n",
      "        (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      "\n",
      "        For evaluating multiple metrics, either give a list of (unique) strings\n",
      "        or a dict with names as keys and callables as values.\n",
      "\n",
      "        NOTE that when using custom scorers, each scorer should return a single\n",
      "        value. Metric functions returning a list/array of values can be wrapped\n",
      "        into multiple scorers that return one value each.\n",
      "\n",
      "        See :ref:`multimetric_grid_search` for an example.\n",
      "\n",
      "        If None, the estimator's score method is used.\n",
      "\n",
      "    n_jobs : int or None, optional (default=None)\n",
      "        Number of jobs to run in parallel.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "\n",
      "    pre_dispatch : int, or string, optional\n",
      "        Controls the number of jobs that get dispatched during parallel\n",
      "        execution. Reducing this number can be useful to avoid an\n",
      "        explosion of memory consumption when more jobs get dispatched\n",
      "        than CPUs can process. This parameter can be:\n",
      "\n",
      "            - None, in which case all the jobs are immediately\n",
      "              created and spawned. Use this for lightweight and\n",
      "              fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "\n",
      "            - An int, giving the exact number of total jobs that are\n",
      "              spawned\n",
      "\n",
      "            - A string, giving an expression as a function of n_jobs,\n",
      "              as in '2*n_jobs'\n",
      "\n",
      "    iid : boolean, default=False\n",
      "        If True, return the average score across folds, weighted by the number\n",
      "        of samples in each test set. In this case, the data is assumed to be\n",
      "        identically distributed across the folds, and the loss minimized is\n",
      "        the total loss per sample, and not the mean loss across the folds.\n",
      "\n",
      "        .. deprecated:: 0.22\n",
      "            Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n",
      "\n",
      "    cv : int, cross-validation generator or an iterable, optional\n",
      "        Determines the cross-validation splitting strategy.\n",
      "        Possible inputs for cv are:\n",
      "\n",
      "        - None, to use the default 5-fold cross validation,\n",
      "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "        - :term:`CV splitter`,\n",
      "        - An iterable yielding (train, test) splits as arrays of indices.\n",
      "\n",
      "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "        other cases, :class:`KFold` is used.\n",
      "\n",
      "        Refer :ref:`User Guide <cross_validation>` for the various\n",
      "        cross-validation strategies that can be used here.\n",
      "\n",
      "        .. versionchanged:: 0.22\n",
      "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "\n",
      "    refit : boolean, string, or callable, default=True\n",
      "        Refit an estimator using the best found parameters on the whole\n",
      "        dataset.\n",
      "\n",
      "        For multiple metric evaluation, this needs to be a string denoting the\n",
      "        scorer that would be used to find the best parameters for refitting\n",
      "        the estimator at the end.\n",
      "\n",
      "        Where there are considerations other than maximum score in\n",
      "        choosing a best estimator, ``refit`` can be set to a function which\n",
      "        returns the selected ``best_index_`` given ``cv_results_``. In that\n",
      "        case, the ``best_estimator_`` and ``best_parameters_`` will be set\n",
      "        according to the returned ``best_index_`` while the ``best_score_``\n",
      "        attribute will not be available.\n",
      "\n",
      "        The refitted estimator is made available at the ``best_estimator_``\n",
      "        attribute and permits using ``predict`` directly on this\n",
      "        ``GridSearchCV`` instance.\n",
      "\n",
      "        Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      "        ``best_score_`` and ``best_params_`` will only be available if\n",
      "        ``refit`` is set and all of them will be determined w.r.t this specific\n",
      "        scorer.\n",
      "\n",
      "        See ``scoring`` parameter to know more about multiple metric\n",
      "        evaluation.\n",
      "\n",
      "        .. versionchanged:: 0.20\n",
      "            Support for callable added.\n",
      "\n",
      "    verbose : integer\n",
      "        Controls the verbosity: the higher, the more messages.\n",
      "\n",
      "    error_score : 'raise' or numeric\n",
      "        Value to assign to the score if an error occurs in estimator fitting.\n",
      "        If set to 'raise', the error is raised. If a numeric value is given,\n",
      "        FitFailedWarning is raised. This parameter does not affect the refit\n",
      "        step, which will always raise the error. Default is ``np.nan``.\n",
      "\n",
      "    return_train_score : boolean, default=False\n",
      "        If ``False``, the ``cv_results_`` attribute will not include training\n",
      "        scores.\n",
      "        Computing training scores is used to get insights on how different\n",
      "        parameter settings impact the overfitting/underfitting trade-off.\n",
      "        However computing the scores on the training set can be computationally\n",
      "        expensive and is not strictly required to select the parameters that\n",
      "        yield the best generalization performance.\n",
      "\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn import svm, datasets\n",
      "    >>> from sklearn.model_selection import GridSearchCV\n",
      "    >>> iris = datasets.load_iris()\n",
      "    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    >>> svc = svm.SVC()\n",
      "    >>> clf = GridSearchCV(svc, parameters)\n",
      "    >>> clf.fit(iris.data, iris.target)\n",
      "    GridSearchCV(estimator=SVC(),\n",
      "                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
      "    >>> sorted(clf.cv_results_.keys())\n",
      "    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      "     'param_C', 'param_kernel', 'params',...\n",
      "     'rank_test_score', 'split0_test_score',...\n",
      "     'split2_test_score', ...\n",
      "     'std_fit_time', 'std_score_time', 'std_test_score']\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    cv_results_ : dict of numpy (masked) ndarrays\n",
      "        A dict with keys as column headers and values as columns, that can be\n",
      "        imported into a pandas ``DataFrame``.\n",
      "\n",
      "        For instance the below given table\n",
      "\n",
      "        +------------+-----------+------------+-----------------+---+---------+\n",
      "        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      "        +============+===========+============+=================+===+=========+\n",
      "        |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
      "        +------------+-----------+------------+-----------------+---+---------+\n",
      "        |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
      "        +------------+-----------+------------+-----------------+---+---------+\n",
      "        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
      "        +------------+-----------+------------+-----------------+---+---------+\n",
      "        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
      "        +------------+-----------+------------+-----------------+---+---------+\n",
      "\n",
      "        will be represented by a ``cv_results_`` dict of::\n",
      "\n",
      "            {\n",
      "            'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      "                                         mask = [False False False False]...)\n",
      "            'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      "                                        mask = [ True  True False False]...),\n",
      "            'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      "                                         mask = [False False  True  True]...),\n",
      "            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
      "            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
      "            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
      "            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
      "            'rank_test_score'    : [2, 4, 3, 1],\n",
      "            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
      "            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
      "            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
      "            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
      "            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      "            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      "            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
      "            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
      "            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      "            }\n",
      "\n",
      "        NOTE\n",
      "\n",
      "        The key ``'params'`` is used to store a list of parameter\n",
      "        settings dicts for all the parameter candidates.\n",
      "\n",
      "        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      "        ``std_score_time`` are all in seconds.\n",
      "\n",
      "        For multi-metric evaluation, the scores for all the scorers are\n",
      "        available in the ``cv_results_`` dict at the keys ending with that\n",
      "        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      "        above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "\n",
      "    best_estimator_ : estimator\n",
      "        Estimator that was chosen by the search, i.e. estimator\n",
      "        which gave highest score (or smallest loss if specified)\n",
      "        on the left out data. Not available if ``refit=False``.\n",
      "\n",
      "        See ``refit`` parameter for more information on allowed values.\n",
      "\n",
      "    best_score_ : float\n",
      "        Mean cross-validated score of the best_estimator\n",
      "\n",
      "        For multi-metric evaluation, this is present only if ``refit`` is\n",
      "        specified.\n",
      "\n",
      "        This attribute is not available if ``refit`` is a function.\n",
      "\n",
      "    best_params_ : dict\n",
      "        Parameter setting that gave the best results on the hold out data.\n",
      "\n",
      "        For multi-metric evaluation, this is present only if ``refit`` is\n",
      "        specified.\n",
      "\n",
      "    best_index_ : int\n",
      "        The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      "        candidate parameter setting.\n",
      "\n",
      "        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      "        the parameter setting for the best model, that gives the highest\n",
      "        mean score (``search.best_score_``).\n",
      "\n",
      "        For multi-metric evaluation, this is present only if ``refit`` is\n",
      "        specified.\n",
      "\n",
      "    scorer_ : function or a dict\n",
      "        Scorer function used on the held out data to choose the best\n",
      "        parameters for the model.\n",
      "\n",
      "        For multi-metric evaluation, this attribute holds the validated\n",
      "        ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      "\n",
      "    n_splits_ : int\n",
      "        The number of cross-validation splits (folds/iterations).\n",
      "\n",
      "    refit_time_ : float\n",
      "        Seconds used for refitting the best model on the whole dataset.\n",
      "\n",
      "        This is present only if ``refit`` is not False.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The parameters selected are those that maximize the score of the left out\n",
      "    data, unless an explicit score is passed in which case it is used instead.\n",
      "\n",
      "    If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      "    point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      "    reasons if individual jobs take very little time, but may raise errors if\n",
      "    the dataset is large and not enough memory is available.  A workaround in\n",
      "    this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      "    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      "    n_jobs`.\n",
      "\n",
      "    See Also\n",
      "    ---------\n",
      "    :class:`ParameterGrid`:\n",
      "        generates all the combinations of a hyperparameter grid.\n",
      "\n",
      "    :func:`sklearn.model_selection.train_test_split`:\n",
      "        utility function to split the data into a development set usable\n",
      "        for fitting a GridSearchCV instance and an evaluation set for\n",
      "        its final evaluation.\n",
      "\n",
      "    :func:`sklearn.metrics.make_scorer`:\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "print(GridSearchCV.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "deepcopy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
