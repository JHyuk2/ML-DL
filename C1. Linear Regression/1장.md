# Chapter1. 머신러닝 개요.

> ML? DL? AI?

1. 머신러닝(Machine Learning)이란?
- One way to implement weak AI
    - computer algorithms that allow computer programs to automatically improve through experience. - Tom Mitchell

2. 딥러닝(Deep Learning)이란?
- Learning Algorithms based on `deep neural networks`, which are composed of cascades of layers of processing units to extract features from data.
  
    <img src="https://www.researchgate.net/profile/Martin_Musiol/publication/308414212/figure/fig1/AS:409040078295040@1474534162122/A-general-model-of-a-deep-neural-network-It-consists-of-an-input-layer-some-here-two.png">



```
# 범주의 크기를 따져보면 아래와 같다.
AI > Machine Learning > Deep Learning
```

## Classis in ML
### Superviesed Learning (지도학습)
- 데이터에 입력 값과 출력 값이 주어진 경우, 입출력값 사이의 관계를 학습하는 머신러닝 알고리즘
- 데이터 형태~ 전체 데이터셋 D = {(x1, y1), (x2, y2), (x3, y3), ...}
- 분류, 회귀

### Unsupervised Learning (비지도학습)



# Chapter 2. 선형회귀분석 (linear regersssion)

````
y1 = a + b1*x1 + b2*x2 + ... + bk*xk + ei
````

독립변수와 종속변수 사이의 관계를 선형의 관계로 가정,

독립변수의 종류

- 양적 입력 (연속값)
- 양적 입력의 변환(log, root)
- 입력 변수의 다항식(2차, 3차 등)

- 두 변수 사이의 교호작용(X_3 = X_1 * X_2)
- 질적인 입력을 위한 dummy variable (one-hot encoding)



## 2-1. 단순회귀분석 (simple regression model)

두 변수 X, Y의 n개의 확률포본에 대한 관계를 다음과 같이 직선으로 가정한다.

```
# i = 1,2, ... ,n
y1 = a + bxi + ei 
```

이때, ei ~ N(0, o^2) 이고, 서로 독립이며, a, b는 미지의 모수이다.

그리고yi 는 종속변수로 독립변수 xi 에 따라 결정되는 값이다.

> 선형회귀의 목적은 a, b를 구하는 것.
>
> 어떤 직선이 가장 잘 설명하느냐를 찾기 위해 `최소제곱법`을 사용한다. ==> 오차의 제곱(거리의 최소화)

```
D = sigma(i=1, n, ei^2) = sigma(i=1, n, (yi - a- bxi)^2)
```

> D를 각각 a와 b에 대해 편미분한 값이 0이 되는 일차방정식을 풀면 된다.



#### 단순 회귀모형의 기본가정

- 1) 두 변수 X, Y간에는 직선관계가 성립되어야 한다.
- 2) 오차들은 평균이 0이고 분산이 o^2인 정규분포를 따라야 한다.
- 3) 오차들의 분산은 o^2로 같아야 한다.
- 4) 오차들은 서로 독립이어야 한다.



하지만 scale에 대한 문제가 나올 수 있기 때문에, R_square(결정계수)값을 구함으로써, 어느 정도 적합한지에 대해 파악할 수 있다.



### 결정계수(coefficient of determination) 

`R2 (R스퀘어)`

데이터들의 변동 중 `회귀 분석에 의해 설명되는 부분(식이 평균으로부터 떨어져있는 거리)`  + `오차의 영역으로 설명되는 부분(실측값과 예측값의 차이)` 를 통해 검증하는데, **오차에 의해 설명되는 부분보다 회귀분석에 의해 설명되는 부분이 클 때** 잘 된 예측이라고 볼 수 있다.

- SST (Sum of Square Total)  - 총변동
- SSR (Sum of Square Regression) - 회귀 변동
- SSE (Sum of Square Error) - 잔차 변동

#### 결정계수 R**2 = SSR / SST (전체 변동 중 회귀변동이 설명하는 비중)



#### 잔차분석

모델이 충분히 잘 적합되었는지를 판정하기 위해서는 반드시 잔차분석을 해야 한다.

가로축을 y^, 세로축을 잔차 e^ 로 하여, 잔차 산점도(scatter plot)을 찍어 보았을 때 만일 잘 적합 되었다면, 어떤 경향도 없이 랜덤하게 잘 분포 할 것.



## 2-2. 다중회귀분석 (multiple linear regression model)

2개 이상의 독립변수와 종속변수와의 관계를 선형으로 가정하는 모형

```
y = Xb + e (단, X, b는 벡터를 의미함)
```

계산하는 방법은 위와 같으니 중요한 것만 짚고 넘어가자.

`b^ = (X.transpose() * X).inverse() * X .transpose() * Y` = `(X'X)^-1 X' Y`

물론, 이 이후에 잔차분석과 R2 값은 꼭 계산해보아야 한다.



## 2-3. 곡선회귀분석 (curvilinear regression model)

그냥 다항회귀분석인데 다중회귀분석을 풀어내듯이 풀어내면 된다.



## 2-4. 다중공선성 문제

회귀분석을 할 때 가장 중요한 문제이며 꼭 해결해야 할 과제이다.

- 설명변수들간의 상관계수가 높을 경우, 회귀계수의 값이 매우 커진다.

- 특정 설명변수가 다른 변수들의 선형결합(linear combination)으로 표현되는 경우
- 회귀계수의 변동성이 커져서 통계량과 모수가 서로 반대 부호를 가질 수 있다. 
- F-통계량이 크나, t-통계량들이 작으면 의심해 볼 수 있다.

> ex) 집 가격 => 집 크기, 방의 개수
>
> 1 - 둘 다 큰 상관계수를 가지며 다른 부호를 가짐 | 크기의 상관계수 1000, 방의 개수 상관계수 -999
>
> 2 - 둘의 계수의 합만 일정하게 유지되며 유의미하지 않은 계수가 표시



### 다중공산성이 존재하는지에 대해 검사하는 방법이 필요하다.

#### A) 분산팽창계수 (Variance Inflation Factor)

나머지 설명변수들로 새로운 회귀식을 추정 후 VIF_j = 1 / (1 - (R_j)^2) >10 이면 의심의 여지가 많다고 본다.



> ## 다중공선성을 없애기는 어렵다.
>
> - 사용되는 독립변수들 사이의 상관관계가 어느 정도 존재하는 것이 일반적
> - 따라서, 다중공선성을 최소화하는 것이 낫다: Forward of Backward selection
> - 예측이 목적인 경우에는 어느 정도 허용될 수 있다.
>
> > Solution
> >
> > 1) 변수 선택
> >
> > 2) 변수 추출 - 차원을 줄임
>
> 
>
> ## 다중공선성 문제의 해결 방안
>
> - 신중한 변수 선택
>   - Step-wise regression: Forward or Backward Selection
> - OLS 추정치의 대안
>   - 능동회귀(Ridge regression)
>   - 주성분 회귀(PCA)



## 2-5. 다중회귀분석 모형의 선택

#### 모형선정 척도(Model selectioni measures) = 적합 결핍(Lack of Fit) + 복잡도(Complexity)

> - 적합 결핍, 복잡도는 모두 적을수록 좋으나 양자는 상충(Trade-off)관계
> - 오캄의 레이저(Ocaam's Razor) : The Principle of Parsimony, MDL Principle

#### AIC (Akaike Information Criterion)

#### BIC (Bayesian Infromation Criterion)

> SSE, p를 낮추는 방법을 고려

#### Mallows'Cp



### 서브셋 선택

- 최소제곱법의 결정적인 약점 두 가지
  - 예측 정확도: 최소제곱법은 종종 편향은 낮지만 분산을 높게 추정
  - 설명력:가장 좋은 효과를 보이는 매우 작은 서브셋은 설명력이 부족
- 최적 서브셋 회귀(Best subset regresssion)
  -  RSS 값이 가장 낮은 예측 모형의 최적 서브셋을 구하는 과정
- 순방향, 역방향, 혼합 단게적 선택
  - 순방향 - Forward
  - 역방향 - Backward
  - 혼합 - Hybrid
  - AIC, F-test등에 기반하여 결정
  - c.f. Forward stagewise regression



